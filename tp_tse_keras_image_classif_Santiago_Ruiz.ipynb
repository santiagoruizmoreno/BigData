{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp_tse_keras_image_classif_Santiago_Ruiz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagoruizmoreno/BigData/blob/main/tp_tse_keras_image_classif_Santiago_Ruiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu6N7RJ7dUoa"
      },
      "source": [
        "# Introduction\n",
        "This practical work is about creation of elementary models in keras.\n",
        "While it focuses on image classification, it illustrates many important functionalities of the keras framework and tries to provide the user with good deep-learning development strategies.\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "Please, read the doc, everything is pretty well explained: https://keras.io/api/\n",
        "\n",
        "Check also the tutos:\n",
        "- https://blog.keras.io/\n",
        "- https://machinelearningmastery.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diDc6LA-ABob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e33d8fe-5b1e-4856-ab38-59caef146951"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYD_C2WumJye"
      },
      "source": [
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, Dense, Dropout, MaxPooling2D, Flatten, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy\n",
        "import os\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC7Bu-qLdcoI"
      },
      "source": [
        "## Setup\n",
        "Before you start, please mount your google drive.\n",
        "For each exercise, create an output directory to store monitoring data and your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QNnu_AgBBBR"
      },
      "source": [
        "OUTDIR = \"/content/drive/My Drive/Classification\"\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.makedirs(OUTDIR)\n",
        "# First, check dimensions of the datasets !\n",
        "\n",
        "# shape of images in cifar10: (32, 32, 3)\n",
        "# shape of images in mnist: (28,28)\n",
        "# shape of images in fashion_mnist: (28, 28)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "hSAAJq8m3guK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_fas, y_train_fas), (x_test_fas, y_test_fas) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "52sij4KD6aUC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "Wge3pH_T6lSd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#each image is composed of 28 pixels by 28 inside the mnist dataset\n",
        "print (len(x_train[1]))\n",
        "print (len(x_train[1][1]))"
      ],
      "metadata": {
        "id": "iB0l5yRX7JDJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4091ab-7b41-4a76-d3e6-ee98bd214722"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each image is composed of \n",
        "print (len(x_train_fas[1]))\n",
        "print (len(x_train_fas[1][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkYaOsIK2Ifc",
        "outputId": "bd1f9f03-c97c-4d17-f969-267d54db0a44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28\n",
            "28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# each image is composed of 32 x 32 pixels\n",
        "print (len(x_train_cifar[1]))\n",
        "print (len(x_train_cifar[1][1]))\n",
        "print (len(x_train_cifar[1][1][1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXJaUL1Q28V9",
        "outputId": "f12cd035-742f-4eec-9d79-762818b64984"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "32\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# That's how the images look like\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt \n",
        "plt.imshow(x_train_cifar[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "nbTlTUgw3KeC",
        "outputId": "b45dbe48-5276-435d-a214-18bd72179fcf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f244e234ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-c46sXwd8Eh"
      },
      "source": [
        "# EX1: Lenet-like classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXrk3AVtfLbS"
      },
      "source": [
        "## 1- Build model architecture\n",
        "The architecture has to be a CLASS called \"Lenet_like\".\n",
        "\n",
        "- I can define the width: number of filters in the first layer.\n",
        "- I can define the depth: number of conv layers.\n",
        "- I can specify the number of classes: output neurons.\n",
        "\n",
        "Lenet_like has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n",
        "\n",
        "Hidden dense layer will have 512 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyZwILwynX9Q"
      },
      "source": [
        "# first model\n",
        "# Lenet-like\n",
        "class Lenet_like:\n",
        "  \"\"\"\n",
        "  Lenet like architecture.\n",
        "  \"\"\"\n",
        "  def __init__(self, width, depth, drop, n_classes):\n",
        "    \"\"\"\n",
        "    Architecture settings.\n",
        "\n",
        "    Arguments:\n",
        "      - width: int, first layer number of convolution filters.\n",
        "      - depth: int, number of convolution layer in the network.\n",
        "      - drop: float, dropout rate.\n",
        "      - n_classes: int, number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    self.width = width\n",
        "    self.depth = depth\n",
        "    self.drop = drop\n",
        "    self.n_classes = n_classes\n",
        "  \n",
        "  def __call__(self, X):\n",
        "    \"\"\"\n",
        "    Call classifier layers on the inputs.\n",
        "    \"\"\"\n",
        "    Y = X\n",
        "\n",
        "  \n",
        "    for k in range(self.depth):\n",
        "      # Appliying the convolutional network and its layers\n",
        "           Conv2D(self.width, (3,3,), activation='relu')(Y)\n",
        "      # Dropping out some connections\n",
        "           Dropout(self.drop)(Y)\n",
        "      # Applying max poooling, which reduces the size of the image\n",
        "           MaxPooling2D(pool_size=(3, 3))(Y)\n",
        "           Conv2D(self.width*2, (3, 3), activation='relu', padding='same')(Y)\n",
        "      # Apply successive convolutions to the input !\n",
        "      # Use the functional API to do so\n",
        "    Flatten()(Y)\n",
        "    Dense(1024, activation='relu')(Y)\n",
        "    Dropout(self.drop)(Y)\n",
        "    Dense(512, activation='relu')(Y)\n",
        "    Dense(self.n_classes, activation='relu')(Y)\n",
        "    \n",
        "    # Perceptron\n",
        "    # This is the classification head of the classifier\n",
        "    ...\n",
        "\n",
        "    return Y\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the parameters of a given object in our class\n",
        "#image_classification = Lenet_like(width = 32, depth  = 1, drop = 0.2, n_classes = 4)"
      ],
      "metadata": {
        "id": "2WVi48jMDkC0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting an input for our model\n",
        "\n",
        "#image_classification(X = (32,32,3))"
      ],
      "metadata": {
        "id": "NkKEMWHTEDBc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8OPc7nifmTm"
      },
      "source": [
        "## 2- A function to create a model with Lenet_like architecture\n",
        "I want the model to be able to fit on the following datasets:\n",
        "\n",
        "- mnist\n",
        "- fashion-mnist\n",
        "- cifar-10\n",
        "\n",
        "Create a function called \"make_lenet_model\" that take the name of one of these dataset as a str.\n",
        "It returns a keras Model object.\n",
        "It should obviously take all arguments to init the Lenet_like architecture.\n",
        "Arguments other than the dataset might have default values.\n",
        "I want to be able to monitor the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFbWjtAB_BDi"
      },
      "source": [
        "def make_lenet_model(dataset,\n",
        "                     width=32,\n",
        "                     depth=2,\n",
        "                     drop=0.25,\n",
        "                     n_classes=10):\n",
        "  \"\"\"\n",
        "  Create a Lenet model adapted to the dimensions of a given dataset.\n",
        "  \"\"\"\n",
        "  if dataset == \"cifar10\":\n",
        "    # dimensions of input are: (32, 32, 4)\n",
        "    X = Input(batch_shape=(32,32,3))\n",
        "  elif dataset == \"mnist\" or dataset == \"fashion_mnist\":\n",
        "    # dimensions of input are: (28, 28)\n",
        "    X = Input(batch_shape=(28,28))\n",
        "  else:\n",
        "    raise NotImplementedError(\"Model not implemented for datastet {}\".format(dataset))\n",
        "  \n",
        "  Y = Lenet_like(width, depth, drop, n_classes)(X)\n",
        "  \n",
        "  model = Model(inputs = X, outputs = make_autoencoder_model('mnist'))\n",
        "  # Remember I wanna monitor accuracy\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
        "                       tf.keras.metrics.FalseNegatives()])\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_lenet_model('mnist')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "3IMpZLfsKZmx",
        "outputId": "c28adf99-7f1b-4cff-8efa-9d5f98ae81af"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4e62ea337310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_lenet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-3c4504d77d06>\u001b[0m in \u001b[0;36mmake_lenet_model\u001b[0;34m(dataset, width, depth, drop, n_classes)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model not implemented for datastet {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLenet_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_autoencoder_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-231669a4a5e8>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# Appliying the convolutional network and its layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m            \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0;31m# Dropping out some connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m            \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[1;32m    228\u001b[0m                          \u001b[0;34m'is incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                          \u001b[0;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv2d_2\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (28, 28)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm0iDMSKgMF3"
      },
      "source": [
        "## 3- Create a fitting function\n",
        "\n",
        "I want a function \"fit_model_on\" with the following arguments:\n",
        "\n",
        "- dataset: str name of the dataset\n",
        "- epochs: number of times you fit on the entire training set\n",
        "- batch_size: number of images to average gradient on\n",
        "\n",
        "The function must create a Lenet model and fit it following these parameters.\n",
        "The function should:\n",
        "\n",
        "- fit the model, obviously\n",
        "- store the model architecture in a .json file in your output directory\n",
        "- store the model's weights in a .h5 file in your output directory\n",
        "- store the fitting metrics loss, validation_loss, accuracy, validation_accuracy under the form of a plot exported in a png file.\n",
        "\n",
        "Run your fitting function of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS61Zmb5JeTB"
      },
      "source": [
        "def fit_model_on(dataset,\n",
        "                 epochs=100,\n",
        "                 batch_size=32,\n",
        "                 n_classes=10):\n",
        "  \n",
        "  model_filename = \"lenet_{}.json\".format(dataset)\n",
        "  weight_filename = \"lenet_{}_weights.h5\".format(dataset)\n",
        "  lossplot_filename = \"lenet_{}_loss.png\".format(dataset)\n",
        "  accplot_filename = \"lenet_{}_accuracy.png\".format(dataset)\n",
        "  \n",
        "  # create your model and call it on your dataset\n",
        "  model = ...\n",
        "  # create a Keras ImageDataGenerator to handle your dataset\n",
        "  datagen = ...\n",
        "  \n",
        "  if dataset == \"cifar10\":\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "  elif dataset == \"mnist\":\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  elif dataset == \"fashion_mnist\":\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "  else:\n",
        "    raise NotImplementedError(\"Model not implemented for datastet {}\".format(dataset))\n",
        "  \n",
        "  # Convert class vectors to binary class matrices (one-hot encoding).\n",
        "  y_train = to_categorical(y_train, n_classes)\n",
        "  y_test = to_categorical(y_test, n_classes)\n",
        "\n",
        "  # Be sure that your training/test data is 'float32'\n",
        "  x_train = ...\n",
        "  x_test = ...\n",
        "  # Be sure that your training/test data are between 0 and 1 (pixel image value)\n",
        "  x_train ...\n",
        "  x_test ...\n",
        "\n",
        "  try:\n",
        "    # Fit with keras using 'datagen', the previously defined image generator\n",
        "    history = model.fit_generator(...)\n",
        "  except KeyboardInterrupt:\n",
        "    print(\"Training interrupted!\")\n",
        "  \n",
        "  # first, save the model\n",
        "  json_str = model.to_json()\n",
        "  model_path = os.path.join(OUTDIR, model_filename)\n",
        "  weight_path = os.path.join(OUTDIR, weight_filename)\n",
        "  with open(model_path, \"w\") as txtfile:\n",
        "    txtfile.write(json_str)\n",
        "  \n",
        "  # then, save the weights\n",
        "  model.save_weights(weight_path)\n",
        "  \n",
        "  # finally, plot and save the metrics\n",
        "  ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXV-9BxNT59c"
      },
      "source": [
        "# fit your model\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxxaUv7qk-Zr"
      },
      "source": [
        "# EX2: Lenet-like auto-encoder.\n",
        "\n",
        "This one is very hard, It is fine if you do not finish.\n",
        "It's about autoencoders, feel free to read more about autoencoders : https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "\n",
        "Autoencoders are a very special kind of model. Input $x$ is an image, output $y$ is an image. The model is supposed to predict $x = y$ ! Usually, you build an autoencoder to create a 'deep' representation of your images : the model break your image into a high level descriptor that is useful to build your image back (the descriptors' space is supposed to be very powerful for clustering and image matching algorithms). The representation is built in an unsupervised way ; if $f$ is the function defined by your model, then you expect $f$ to be such as:\n",
        "$$f(x) = y;$$\n",
        "$$x = y$$\n",
        "So you try to minimize $||f(x) - x||^2$ (mean squared error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_IaGLkclOmM"
      },
      "source": [
        "## 1- Build model architecture\n",
        "The architecture has to be a CLASS called \"Autoencoder\".\n",
        "\n",
        "Basically, an autoencoder has two parts:\n",
        "- the Encoder $(conv2D-Maxpooling)\\times n$\n",
        "- the Decoder $(conv2D-Upsampling)\\times n$\n",
        "\n",
        "\n",
        "Autoencoder has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n",
        "\n",
        "------------------------------\n",
        "\n",
        "For the Encoder:\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n",
        "\n",
        "------------------------------\n",
        "For the Decoder:\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to increase the spatial size by a factor of 2 in each direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCG3NvJJpy8B"
      },
      "source": [
        "from keras.layers import BatchNormalization, UpSampling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ8Io8DFp0sI"
      },
      "source": [
        "OUTDIR = \"/content/drive/My Drive/Autoencoding\"\n",
        "if not os.path.exists(OUTDIR):\n",
        "  os.makedirs(OUTDIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2XEJ0oTESRR"
      },
      "source": [
        "# first model\n",
        "# Lenet-like\n",
        "class Autoencoder:\n",
        "  \"\"\"\n",
        "  Aurtoencoder architecture.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Architecture settings.\n",
        "    \"\"\"\n",
        "    # nothing to do in the init.\n",
        "  \n",
        "  def __call__(self, X):\n",
        "    \"\"\"\n",
        "    Call autoencoder layers on the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    # encode\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    # decode\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "    Y = ...\n",
        "\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyQr2CKeqk8b"
      },
      "source": [
        "## 2- A function to create a model with your Autoencoder architecture\n",
        "I want the model to be able to fit on the following datasets:\n",
        "\n",
        "- cifar-10\n",
        "\n",
        "Create a function called \"make_autoencoder_model\" that take no argument.\n",
        "It only returns a keras Model object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiF9FQEBqbHB"
      },
      "source": [
        "def make_autoencoder_model():\n",
        "  \"\"\"\n",
        "  Create and compile autoencoder keras model.\n",
        "  \"\"\"\n",
        "  X = Input(batch_shape=(...))\n",
        "  Y = Autoencoder()(X)\n",
        "  model = Model(inputs=X, outputs=Y)\n",
        "  model.compile(optimizer='adam', metrics=['accuracy'], loss='mean_squared_error')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIk0P030rHwK"
      },
      "source": [
        "Useful function to plot the resulting image produced by your autoencoder.\n",
        "Read them carefully if you want to plot your output correctly (especially 'comparison')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk8X5Vm4rGEr"
      },
      "source": [
        "def stack_horizontally(min, max, images):\n",
        "    return numpy.hstack(images[i] for i in range(min, max))\n",
        "\n",
        "def stack_vertically(length, height, images):\n",
        "    return numpy.vstack(stack_horizontally(i * length, (i + 1) * length, images) for i in range(height))\n",
        "\n",
        "def comparison(inputimgs, outputimgs, length, height):\n",
        "    A = stack_vertically(length, height, inputimgs)\n",
        "    B = stack_vertically(length, height, outputimgs)\n",
        "    C = numpy.ones((A.shape[0], 32, 3))\n",
        "    return numpy.hstack((A, C, B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muPRC6hfseSH"
      },
      "source": [
        "## 3- Create a fitting function\n",
        "\n",
        "I want a function \"fit_model_cifar10\" with the following arguments:\n",
        "\n",
        "- dataset: str name of the dataset\n",
        "- n_epochs: number of times you fit on the entire training set\n",
        "- batch_size: number of images to average gradient on\n",
        "- visualization_size (square root of number of test pred you wanna show to check your result)\n",
        "- verbose (debug purpose to see tensor dimensions in your architecture)\n",
        "\n",
        "The function must create an autoencoder model and fit it following these parameters.\n",
        "The function should:\n",
        "\n",
        "- fit the model, obviously\n",
        "- store the model architecture in a .json file in your output directory\n",
        "- store the model's weights in a .h5 file in your output directory\n",
        "- store the fitting metrics loss, validation_loss, under the form of a plot exported in a png file.\n",
        "- store the rebuilt test images under the form of a plot exported in a png file.\n",
        "\n",
        "Run your fitting function of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yXeUGocrRDp"
      },
      "source": [
        "def fit_model_on_cifar10(n_epochs=3, batch_size=128, visualization_size=5, verbose=1):\n",
        "\n",
        "  model_filename = \"autoencoder_cifar10.json\"\n",
        "  weight_filename = \"autoencoder_cifar10_weights.h5\"\n",
        "  lossplot_filename = \"autoencoder_cifar10_loss.png\"\n",
        "  visuplot_filename = \"autoencoder_cifar10_visu.png\"\n",
        "  # create your model and call it on your dataset\n",
        "  model = ...\n",
        "  if verbose > 0:\n",
        "    print(model.summary())\n",
        "  (x_train, _), (x_test, _) = cifar10.load_data()\n",
        "  # Be sure that your training/test data is 'float32'\n",
        "  x_train = ...\n",
        "  x_test = ...\n",
        "  # Be sure that your training/test data are between 0 and 1 (pixel image value)\n",
        "  x_train ...\n",
        "  x_test ...\n",
        "  try:\n",
        "    history = model.fit(...)\n",
        "  except KeyboardInterrupt:\n",
        "    print(\"Training interrupted!\")\n",
        "  \n",
        "  # first, save the model\n",
        "  json_str = model.to_json()\n",
        "  model_path = os.path.join(OUTDIR, model_filename)\n",
        "  weight_path = os.path.join(OUTDIR, weight_filename)\n",
        "  with open(model_path, \"w\") as txtfile:\n",
        "    txtfile.write(json_str)\n",
        "  # then, save the weights\n",
        "  model.save_weights(weight_path)\n",
        "  # now, save metrics plots\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQEtpB9rsU_2"
      },
      "source": [
        "# fit your model\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}